{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDZ17nGB8tJ-"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This Colab showcases a multi-stage ADK agent pipeline that automates the transition from initial concept to a refined short story. The workflow includes:\n",
        "\n",
        "- Prompt Optimization: An `Enhancer` agent refines the user's input for maximum clarity.\n",
        "\n",
        "- Parallel Generation: Two distinct writing agents‚Äî`Creative` and `Focused`‚Äîsimultaneously produce unique drafts.\n",
        "\n",
        "- Expert Review: An `Evaluator` agent performs a comparative analysis, selecting the superior version and providing a detailed rationale for its choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCr0_MreluEq"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Q_Ammn1SihT3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install google-adk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "S792mL06ioB6"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "from pydantic import BaseModel, Field\n",
        "import os\n",
        "import pprint\n",
        "\n",
        "from google.adk.agents.llm_agent import LlmAgent\n",
        "from google.adk.agents.parallel_agent import ParallelAgent\n",
        "from google.adk.agents.sequential_agent import SequentialAgent\n",
        "from google.adk.runners import InMemoryRunner, Runner\n",
        "from google.genai import types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CawlSa3lzTz"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "c8OCQG25tfSe"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "GEMINI_API_KEY = \"GEMINI_API_KEY\" # @param {\"type\":\"string\"}\n",
        "\n",
        "GEMINI_MODEL = \"gemini-2.5-flash\" # @param {\"type\":\"string\"}\n",
        "APP_NAME = \"story_contest_app\"  # @param {\"type\":\"string\"}\n",
        "USER_ID = \"user_123\"  # @param {\"type\":\"string\"}\n",
        "SESSION_ID = \"story_session_v1\"  # @param {\"type\":\"string\"}\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = GEMINI_API_KEY\n",
        "os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = 'False'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaD1ld9Yl2Lm"
      },
      "source": [
        "### Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "ywMdlf2h42d7"
      },
      "outputs": [],
      "source": [
        "ENHANCER_AGENT_PROMPT = \"\"\"\n",
        "You are an expert Prompt Engineer.\n",
        "Your goal is to take the user's input and improve it by adding necessary clarity, slight detail, or constraints to make it a better input for a debate.\n",
        "Keep the enhanced prompt concise but clear.\n",
        "Output ONLY the enhanced prompt text.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "gURa3Mc2ivpb"
      },
      "outputs": [],
      "source": [
        "POSITIVE_AGENT_PROMPT = \"\"\"\n",
        "You are a debator in favor of the prompt.\n",
        "Write a VERY SHORT argument supporting this prompt:\n",
        "\n",
        "__PROMPT_STARTS__\n",
        "\n",
        "{enhanced_prompt}\n",
        "\n",
        "__PROMPT_ENDS__\n",
        "\n",
        "Output ONLY the argument text.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "YHhRZRuTiynt"
      },
      "outputs": [],
      "source": [
        "NEGATIVE_AGENT_PROMPT = \"\"\"\n",
        "You are a debator not in favor this prompt.\n",
        "Write a VERY SHORT argument debating against this prompt:\n",
        "\n",
        "__PROMPT_STARTS__\n",
        "\n",
        "{enhanced_prompt}\n",
        "\n",
        "__PROMPT_ENDS__\n",
        "\n",
        "Output ONLY the argument text.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "emmbhDdMi1Bs"
      },
      "outputs": [],
      "source": [
        "EVALUATOR_AGENT_PROMPT = \"\"\"\n",
        "You are a master moderator. Evaluate these two arguments based on the user's original prompt:\n",
        "\n",
        "__PROMPT_STARTS__\n",
        "{enhanced_prompt}\n",
        "__PROMPT_ENDS__\n",
        "\n",
        "__ARGUMENT_1_STARTS__\n",
        "{positive_argument}\n",
        "__ARGUMENT_1_ENDS__\n",
        "\n",
        "__ARGUMENT_2_STARTS__\n",
        "{negative_argument}\n",
        "__ARGUMENT_2_ENDS__\n",
        "\n",
        "Task: Decide which argument is more convincing.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PCDVo4dl4tu"
      },
      "source": [
        "## Data Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "5vCas7Gbi27E"
      },
      "outputs": [],
      "source": [
        "# --- 1. Define Output Schema for the Evaluator ---\n",
        "class EvaluationResult(BaseModel):\n",
        "    winner: str = Field(description=\"The name of the winning agent (e.g., 'positive' or 'negative').\")\n",
        "    rationale: str = Field(description=\"A brief explanation of why this argument was chosen.\")\n",
        "    best_argument_text: str = Field(description=\"The complete text of the winning argument.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XVWJ6cSpovW"
      },
      "source": [
        "## Agents\n",
        "\n",
        "The following diagram illustrates the agents within this Colab and their interaction patterns.\n",
        "\n",
        "![graphviz.svg](data:image/svg+xml;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "t_lJIg0X4-fP"
      },
      "outputs": [],
      "source": [
        "# Prompt Enhancer Agent\n",
        "enhancer_agent = LlmAgent(\n",
        "    name=\"Enhancer\",\n",
        "    model=GEMINI_MODEL,\n",
        "    instruction=ENHANCER_AGENT_PROMPT,\n",
        "    # It will automatically use the incoming message as input.\n",
        "    # It will write its response to this key for the next agents to use.\n",
        "    output_key=\"enhanced_prompt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "_ZI2XOkAi4qa"
      },
      "outputs": [],
      "source": [
        "# positive Agent\n",
        "positive_writer = LlmAgent(\n",
        "    name=\"PositiveWriter\",\n",
        "    model=GEMINI_MODEL,\n",
        "    instruction=POSITIVE_AGENT_PROMPT,\n",
        "    generate_content_config=types.GenerateContentConfig,\n",
        "    output_key=\"positive_argument\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "Vw6zh4ARi6eL"
      },
      "outputs": [],
      "source": [
        "# Negative Agent\n",
        "negative_writer = LlmAgent(\n",
        "    name=\"NegativeWriter\",\n",
        "    model=GEMINI_MODEL,\n",
        "    instruction=NEGATIVE_AGENT_PROMPT,\n",
        "    generate_content_config=types.GenerateContentConfig,\n",
        "    output_key=\"negative_argument\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "5AkvNW8ni9pZ"
      },
      "outputs": [],
      "source": [
        "# Parallel Agent\n",
        "parallel_writers = ParallelAgent(\n",
        "    name=\"ParallelWriters\",\n",
        "    sub_agents=[positive_writer, negative_writer],\n",
        "    description=\"Runs two different argument personas in parallel.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "zA1qFF0LjAD0"
      },
      "outputs": [],
      "source": [
        "# Evaluator Agent\n",
        "evaluator_agent = LlmAgent(\n",
        "    name=\"Evaluator\",\n",
        "    model=GEMINI_MODEL,\n",
        "    instruction=EVALUATOR_AGENT_PROMPT,\n",
        "    output_schema=EvaluationResult,\n",
        "    output_key=\"final_evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "YRa2ZPs-jH1a"
      },
      "outputs": [],
      "source": [
        "# Root, Sequential Agent\n",
        "root_agent = SequentialAgent(\n",
        "    name=\"ModeratorAgent\",\n",
        "    sub_agents=[enhancer_agent, parallel_writers, evaluator_agent],\n",
        "    description=\"Generates two arguments in parallel and then evaluates them.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtXvtYtMl930"
      },
      "source": [
        "## Running"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW07kKsJ8Tw6"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "JeEux3p16wvP"
      },
      "outputs": [],
      "source": [
        "async def create_runner_session(app_name: str, user_id: str, session_id: str) -> Runner:\n",
        "  runner = InMemoryRunner(agent=root_agent, app_name=app_name)\n",
        "\n",
        "  await runner.session_service.create_session(\n",
        "      app_name=app_name, user_id=user_id, session_id=session_id,\n",
        "  )\n",
        "  return runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "lKCHufscjKLv"
      },
      "outputs": [],
      "source": [
        "async def run_story_pipeline(runner: Runner, prompt: str):\n",
        "    print(f\"--- Starting argument Pipeline for: '{prompt}' ---\\n\")\n",
        "    runner = InMemoryRunner(agent=root_agent, app_name=APP_NAME)\n",
        "\n",
        "    await runner.session_service.create_session(\n",
        "        app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        "    )\n",
        "\n",
        "    message = types.Content(role='user', parts=[types.Part(text=prompt)])\n",
        "\n",
        "    async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=message):\n",
        "\n",
        "        author = event.author or \"System\"\n",
        "        if event.is_final_response() and event.content and event.content.parts:\n",
        "             print(f\"[{author}] completed step.\")\n",
        "\n",
        "    session = await runner.session_service.get_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "    eval_data = session.state.get(\"final_evaluation\")\n",
        "\n",
        "    if eval_data:\n",
        "        try:\n",
        "            # Use model_validate because eval_data is likely already a dict\n",
        "            if isinstance(eval_data, dict):\n",
        "                 result = EvaluationResult.model_validate(eval_data)\n",
        "            else:\n",
        "                 # Fallback just in case it is returned as a string sometimes\n",
        "                 result = EvaluationResult.model_validate_json(eval_data)\n",
        "\n",
        "            print(\"\\n=== üèÜ FINAL VERDICT ===\")\n",
        "            pprint.pprint(f\"WINNER: {result.winner}\")\n",
        "            pprint.pprint(f\"RATIONALE: {result.rationale}\")\n",
        "            pprint.pprint(f\"WINNING ARGUMENT:\\n\\\"{result.best_argument_text}\\\"\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError parsing final data: {e}\\nRaw output type: {type(eval_data)}\\nRaw output: {eval_data}\")\n",
        "    else:\n",
        "        print(\"\\nError: No evaluation found in state.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2treiwon8XXJ"
      },
      "source": [
        "### Running the Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "rk4T4hpr8Nru"
      },
      "outputs": [],
      "source": [
        "runner = await create_runner_session(APP_NAME, USER_ID, SESSION_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "-T1jBQqooapj"
      },
      "outputs": [],
      "source": [
        "# @title  {\"run\":\"auto\",\"vertical-output\":true}\n",
        "PROMPT = \"8 hours of sleep is sufficient for a high school student\"  # @param {\"type\":\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzHEzakqjmzg",
        "outputId": "6c737505-4ff4-4548-ca88-a85a5b1cdce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting argument Pipeline for: '8 hours of sleep is sufficient for a high school student' ---\n",
            "\n",
            "[Enhancer] completed step.\n",
            "[PositiveWriter] completed step.\n",
            "[NegativeWriter] completed step.\n",
            "[Evaluator] completed step.\n",
            "\n",
            "=== üèÜ FINAL VERDICT ===\n",
            "'WINNER: NegativeWriter'\n",
            "(\"RATIONALE: The NegativeWriter's argument is more convincing because it \"\n",
            " 'introduces the critical concept of individual variability in sleep needs, '\n",
            " \"directly challenging the 'universally sufficient' aspect of the prompt. \"\n",
            " 'While 8 hours is a common guideline, asserting its universality for all high '\n",
            " \"school students' optimal performance and well-being is often not supported \"\n",
            " 'by reality, as many may require more. This nuanced perspective makes it more '\n",
            " 'realistic and compelling.')\n",
            "('WINNING ARGUMENT:\\n'\n",
            " '\"Optimal sleep is not universal; many high school students require more than '\n",
            " '8 hours for peak performance and true well-being.\"')\n"
          ]
        }
      ],
      "source": [
        "await run_story_pipeline(runner, PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "fgSmD-yYFJde"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sCr0_MreluEq"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
